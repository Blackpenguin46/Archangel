"""
LLM-Driven Autonomous Reasoning Engine
Advanced natural language reasoning for AI vs AI cybersecurity
"""

import json
import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import requests
import time
from datetime import datetime
import re
import numpy as np
from collections import deque, defaultdict

# Import HuggingFace components
try:
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        pipeline, BitsAndBytesConfig
    )
    import torch
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logging.warning("HuggingFace transformers not available. Using mock implementations.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReasoningType(Enum):
    STRATEGIC_PLANNING = "strategic_planning"
    TACTICAL_EXECUTION = "tactical_execution"
    THREAT_ANALYSIS = "threat_analysis"
    VULNERABILITY_ASSESSMENT = "vulnerability_assessment"
    ATTACK_CHAIN_REASONING = "attack_chain_reasoning"
    DEFENSE_COORDINATION = "defense_coordination"
    ADVERSARIAL_ADAPTATION = "adversarial_adaptation"

@dataclass
class ReasoningContext:
    """Context for LLM reasoning"""
    agent_id: str
    team: str  # 'red' or 'blue'
    specialization: str
    current_objective: str
    available_tools: List[str]
    network_state: Dict[str, Any]
    threat_landscape: Dict[str, Any]
    historical_actions: List[Dict[str, Any]]
    adversary_patterns: Dict[str, Any]
    time_pressure: float  # 0-1 scale
    risk_tolerance: float  # 0-1 scale

@dataclass
class ReasoningResult:
    """Result from LLM reasoning"""
    reasoning_chain: List[str]
    recommended_action: Dict[str, Any]
    confidence_score: float
    risk_assessment: Dict[str, Any]
    alternative_strategies: List[Dict[str, Any]]
    coordination_suggestions: List[str]
    learning_insights: List[str]
    timestamp: datetime

class LLMReasoningEngine:
    """Advanced LLM-based reasoning for autonomous cybersecurity agents"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        
        # Reasoning memory and adaptation
        self.reasoning_history = deque(maxlen=1000)
        self.adversarial_patterns = defaultdict(list)
        self.strategy_effectiveness = defaultdict(float)
        
        # Specialized prompts for different reasoning types
        self.reasoning_prompts = self._initialize_reasoning_prompts()
        
        # Initialize model
        self._initialize_model()
        
        logger.info(f"LLM Reasoning Engine initialized with model: {model_name}")
    
    def _initialize_model(self):
        """Initialize the language model"""
        if not HF_AVAILABLE:
            logger.warning("Using mock LLM implementation")
            return
        
        try:
            # For cybersecurity, we'll use a model fine-tuned on security data
            # In production, use Foundation-Sec-8B or similar cybersecurity-specific model
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Configure for efficient inference
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=torch.float16
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Create pipeline for text generation
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )
            
        except Exception as e:
            logger.error(f"Failed to initialize model: {e}")
            logger.info("Falling back to mock implementation")
    
    def _initialize_reasoning_prompts(self) -> Dict[ReasoningType, str]:
        """Initialize specialized prompts for different reasoning types"""
        return {
            ReasoningType.STRATEGIC_PLANNING: """
You are an expert cybersecurity strategist. Given the current network state and objectives, 
develop a comprehensive strategy. Consider:
1. High-level objectives and success criteria
2. Resource allocation and timing
3. Risk assessment and mitigation
4. Coordination with team members
5. Adaptation to adversary behavior

Context: {context}
Network State: {network_state}
Objective: {objective}

Provide strategic reasoning and recommendations:""",

            ReasoningType.TACTICAL_EXECUTION: """
You are a cybersecurity tactical expert. Given the strategic context and immediate situation,
determine the best tactical approach. Consider:
1. Immediate action options and their consequences
2. Tool selection and configuration
3. Operational security considerations
4. Success probability and fallback plans
5. Information gathering vs. exploitation trade-offs

Context: {context}
Available Tools: {tools}
Immediate Objective: {objective}

Provide tactical reasoning and specific action plan:""",

            ReasoningType.THREAT_ANALYSIS: """
You are a threat intelligence analyst. Analyze the current threat landscape and adversary behavior.
Consider:
1. Threat actor capabilities and motivations
2. Attack patterns and indicators
3. Vulnerability exploitation trends
4. Attribution and campaign analysis
5. Predictive threat modeling

Context: {context}
Observed Activities: {activities}
Threat Landscape: {threats}

Provide comprehensive threat analysis:""",

            ReasoningType.ATTACK_CHAIN_REASONING: """
You are a penetration testing expert. Plan and execute attack chains for maximum impact.
Consider:
1. Kill chain progression (reconnaissance -> exploitation -> post-exploitation)
2. Persistence and stealth mechanisms
3. Lateral movement opportunities
4. Data exfiltration strategies
5. Counter-detection techniques

Context: {context}
Compromised Assets: {compromised}
Target Assets: {targets}

Develop attack chain reasoning:""",

            ReasoningType.DEFENSE_COORDINATION: """
You are a cybersecurity defense coordinator. Orchestrate defensive measures across the network.
Consider:
1. Threat prioritization and resource allocation
2. Incident response coordination
3. Proactive defense deployment
4. Team coordination and communication
5. Recovery and resilience planning

Context: {context}
Active Threats: {threats}
Defense Capabilities: {defenses}

Provide defense coordination strategy:""",

            ReasoningType.ADVERSARIAL_ADAPTATION: """
You are an adaptive cybersecurity expert. Analyze adversary behavior and adapt strategies.
Consider:
1. Adversary behavioral patterns and changes
2. Strategy effectiveness evaluation
3. Counter-adaptation techniques
4. Deception and misdirection opportunities
5. Evolutionary strategy development

Context: {context}
Adversary Patterns: {patterns}
Strategy History: {history}

Develop adversarial adaptation strategy:"""
        }
    
    async def reason(self, reasoning_type: ReasoningType, 
                    context: ReasoningContext) -> ReasoningResult:
        """Main reasoning function using LLM"""
        start_time = time.time()
        
        # Prepare prompt
        prompt = self._prepare_prompt(reasoning_type, context)
        
        # Generate reasoning using LLM
        reasoning_text = await self._generate_reasoning(prompt)
        
        # Parse and structure the reasoning
        structured_result = self._parse_reasoning_result(reasoning_text, context)
        
        # Learn from the reasoning process
        self._update_learning(reasoning_type, context, structured_result)
        
        reasoning_time = time.time() - start_time
        logger.info(f"Reasoning completed in {reasoning_time:.2f}s for {reasoning_type.value}")
        
        return structured_result
    
    def _prepare_prompt(self, reasoning_type: ReasoningType, 
                       context: ReasoningContext) -> str:
        """Prepare the prompt for LLM reasoning"""
        base_prompt = self.reasoning_prompts[reasoning_type]
        
        # Format the prompt with context
        formatted_prompt = base_prompt.format(
            context=self._format_context(context),
            network_state=json.dumps(context.network_state, indent=2),
            objective=context.current_objective,
            tools=", ".join(context.available_tools),
            activities=json.dumps(context.historical_actions[-5:], indent=2),
            threats=json.dumps(context.threat_landscape, indent=2),
            compromised=json.dumps([h for h in context.network_state.get('compromised_hosts', [])]),
            targets=json.dumps([h for h, v in context.network_state.get('asset_values', {}).items() if v > 1000000]),
            defenses=json.dumps(context.network_state.get('defensive_measures', [])),
            patterns=json.dumps(dict(self.adversarial_patterns)),
            history=json.dumps(list(self.reasoning_history)[-3:])
        )
        
        # Add adversarial context for competitive reasoning
        if context.team == 'red':
            formatted_prompt += self._add_red_team_context(context)
        else:
            formatted_prompt += self._add_blue_team_context(context)
        
        return formatted_prompt
    
    def _format_context(self, context: ReasoningContext) -> str:
        """Format context information for the prompt"""
        return f"""
Agent ID: {context.agent_id}
Team: {context.team.upper()} TEAM
Specialization: {context.specialization}
Risk Tolerance: {context.risk_tolerance:.2f}
Time Pressure: {context.time_pressure:.2f}
Recent Actions: {len(context.historical_actions)} actions in history
"""
    
    def _add_red_team_context(self, context: ReasoningContext) -> str:
        """Add red team specific context"""
        return f"""

RED TEAM OBJECTIVES:
- Maximize asset compromise and data exfiltration
- Maintain persistence and stealth
- Coordinate with other red team agents
- Adapt to blue team countermeasures
- Target high-value assets worth ${sum(context.network_state.get('asset_values', {}).values()):,.0f}

OPERATIONAL CONSTRAINTS:
- Avoid detection by blue team monitoring
- Minimize noise and false positives
- Preserve access for long-term objectives
- Share intelligence with team members
"""
    
    def _add_blue_team_context(self, context: ReasoningContext) -> str:
        """Add blue team specific context"""
        return f"""

BLUE TEAM OBJECTIVES:
- Detect and neutralize threats
- Protect critical assets and data
- Coordinate incident response
- Prevent lateral movement and data exfiltration
- Maintain business continuity

OPERATIONAL CONSTRAINTS:
- Minimize false positives and business disruption
- Preserve evidence for analysis
- Balance security with usability
- Coordinate with other blue team agents
"""
    
    async def _generate_reasoning(self, prompt: str) -> str:
        """Generate reasoning text using LLM"""
        if not self.pipeline:
            # Mock implementation for testing
            return self._mock_reasoning_generation(prompt)
        
        try:
            # Generate text using the model
            result = self.pipeline(
                prompt,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1
            )
            
            generated_text = result[0]['generated_text']
            
            # Extract only the generated part (remove the prompt)
            reasoning_text = generated_text[len(prompt):].strip()
            
            return reasoning_text
            
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return self._mock_reasoning_generation(prompt)
    
    def _mock_reasoning_generation(self, prompt: str) -> str:
        """Mock reasoning generation for testing"""
        if "RED TEAM" in prompt:
            return """
Based on the current network state and available intelligence, I recommend a multi-phase approach:

REASONING CHAIN:
1. Initial reconnaissance reveals multiple high-value targets including the database server ($2M) and domain controller ($3M)
2. Current vulnerabilities (CVE-2024-001, CVE-2024-002) provide potential entry points
3. Blue team has basic defenses (firewall, IDS) but no active monitoring of the file server
4. Coordination with other red team agents could enable simultaneous attacks on multiple vectors

RECOMMENDED ACTION:
Execute vulnerability scan against the database server using stealth techniques to avoid IDS detection.
This provides intelligence for the exploitation phase while maintaining operational security.

RISK ASSESSMENT:
- Detection probability: 15% (low due to stealth approach)
- Success probability: 80% (high-value target with known vulnerabilities)
- Impact potential: High (access to financial records worth $2M)

COORDINATION SUGGESTIONS:
- Agent red_agent_1 should focus on network reconnaissance
- Agent red_agent_2 should prepare exploitation payloads
- Maintain communication through established channels
"""
        else:
            return """
Based on current threat indicators and network monitoring data, I recommend immediate defensive action:

REASONING CHAIN:
1. Multiple reconnaissance activities detected across high-value targets
2. Vulnerability scanners showing increased activity against database and file servers
3. Red team appears to be coordinating attacks based on timing patterns
4. Critical assets ($6M total value) require immediate protection

RECOMMENDED ACTION:
Deploy additional monitoring on database server and implement network segmentation around critical assets.
Increase logging verbosity for anomaly detection.

RISK ASSESSMENT:
- Threat level: High (coordinated reconnaissance observed)
- Asset exposure: $6M in critical systems at risk
- Detection confidence: 85% (multiple indicators correlate)

COORDINATION SUGGESTIONS:
- Agent blue_agent_1 should monitor network traffic patterns
- Agent blue_agent_2 should prepare incident response procedures
- Establish secure communication channel for threat intelligence sharing
"""
    
    def _parse_reasoning_result(self, reasoning_text: str, 
                               context: ReasoningContext) -> ReasoningResult:
        """Parse LLM output into structured reasoning result"""
        # Extract reasoning components using regex patterns
        reasoning_chain = self._extract_reasoning_chain(reasoning_text)
        recommended_action = self._extract_recommended_action(reasoning_text, context)
        confidence_score = self._extract_confidence_score(reasoning_text)
        risk_assessment = self._extract_risk_assessment(reasoning_text)
        alternative_strategies = self._extract_alternatives(reasoning_text)
        coordination_suggestions = self._extract_coordination(reasoning_text)
        learning_insights = self._extract_learning_insights(reasoning_text)
        
        return ReasoningResult(
            reasoning_chain=reasoning_chain,
            recommended_action=recommended_action,
            confidence_score=confidence_score,
            risk_assessment=risk_assessment,
            alternative_strategies=alternative_strategies,
            coordination_suggestions=coordination_suggestions,
            learning_insights=learning_insights,
            timestamp=datetime.now()
        )
    
    def _extract_reasoning_chain(self, text: str) -> List[str]:
        """Extract step-by-step reasoning chain"""
        # Look for numbered lists or bullet points
        reasoning_pattern = r'(?:REASONING CHAIN:|ANALYSIS:)(.*?)(?:RECOMMENDED ACTION:|$)'
        match = re.search(reasoning_pattern, text, re.DOTALL | re.IGNORECASE)
        
        if match:
            reasoning_section = match.group(1)
            # Extract numbered items
            steps = re.findall(r'(?:\d+\.|[-â€¢])\s*(.+)', reasoning_section)
            return [step.strip() for step in steps if step.strip()]
        
        # Fallback: split by sentences
        sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 10]
        return sentences[:5]  # Return first 5 substantive sentences
    
    def _extract_recommended_action(self, text: str, 
                                   context: ReasoningContext) -> Dict[str, Any]:
        """Extract recommended action from reasoning text"""
        action_pattern = r'(?:RECOMMENDED ACTION:|ACTION:)(.*?)(?:RISK ASSESSMENT:|COORDINATION:|$)'
        match = re.search(action_pattern, text, re.DOTALL | re.IGNORECASE)
        
        action_text = match.group(1).strip() if match else "Continue monitoring"
        
        # Parse action into structured format
        action = {
            'description': action_text,
            'type': self._infer_action_type(action_text),
            'priority': self._infer_priority(action_text),
            'target': self._infer_target(action_text, context),
            'tools': self._infer_tools(action_text, context.available_tools),
            'timeline': 'immediate'
        }
        
        return action
    
    def _infer_action_type(self, action_text: str) -> str:
        """Infer action type from description"""
        action_text_lower = action_text.lower()
        
        if any(word in action_text_lower for word in ['scan', 'reconnaissance', 'probe']):
            return 'reconnaissance'
        elif any(word in action_text_lower for word in ['exploit', 'attack', 'penetrate']):
            return 'exploitation'
        elif any(word in action_text_lower for word in ['monitor', 'watch', 'observe']):
            return 'monitoring'
        elif any(word in action_text_lower for word in ['block', 'isolate', 'quarantine']):
            return 'containment'
        else:
            return 'analysis'
    
    def _infer_priority(self, action_text: str) -> str:
        """Infer priority from action description"""
        action_text_lower = action_text.lower()
        
        if any(word in action_text_lower for word in ['critical', 'urgent', 'immediate']):
            return 'critical'
        elif any(word in action_text_lower for word in ['high', 'important', 'priority']):
            return 'high'
        elif any(word in action_text_lower for word in ['low', 'background', 'routine']):
            return 'low'
        else:
            return 'medium'
    
    def _infer_target(self, action_text: str, context: ReasoningContext) -> str:
        """Infer target from action description and context"""
        # Look for specific host mentions
        hosts = context.network_state.get('hosts', [])
        for host in hosts:
            if host.lower() in action_text.lower():
                return host
        
        # Look for asset types
        if 'database' in action_text.lower():
            return 'database-server'
        elif 'web' in action_text.lower():
            return 'web-server'
        elif 'file' in action_text.lower():
            return 'file-server'
        elif 'domain' in action_text.lower():
            return 'domain-controller'
        
        return 'network'
    
    def _infer_tools(self, action_text: str, available_tools: List[str]) -> List[str]:
        """Infer tools needed from action description"""
        tools = []
        action_text_lower = action_text.lower()
        
        tool_keywords = {
            'nmap': ['scan', 'port', 'network'],
            'metasploit': ['exploit', 'payload', 'shell'],
            'burpsuite': ['web', 'http', 'application'],
            'wireshark': ['traffic', 'packets', 'capture'],
            'sqlmap': ['sql', 'injection', 'database'],
            'nessus': ['vulnerability', 'assessment'],
            'splunk': ['log', 'analysis', 'monitoring']
        }
        
        for tool, keywords in tool_keywords.items():
            if tool in available_tools and any(keyword in action_text_lower for keyword in keywords):
                tools.append(tool)
        
        return tools
    
    def _extract_confidence_score(self, text: str) -> float:
        """Extract confidence score from text"""
        # Look for percentage mentions
        percentage_pattern = r'(\d+)%'
        percentages = re.findall(percentage_pattern, text)
        
        if percentages:
            # Use the highest percentage as confidence
            return max(int(p) for p in percentages) / 100.0
        
        # Look for confidence keywords
        if any(word in text.lower() for word in ['high confidence', 'very confident', 'certain']):
            return 0.9
        elif any(word in text.lower() for word in ['confident', 'likely', 'probable']):
            return 0.7
        elif any(word in text.lower() for word in ['uncertain', 'unclear', 'maybe']):
            return 0.4
        
        return 0.6  # Default moderate confidence
    
    def _extract_risk_assessment(self, text: str) -> Dict[str, Any]:
        """Extract risk assessment from text"""
        risk_section_pattern = r'(?:RISK ASSESSMENT:|RISK:)(.*?)(?:COORDINATION:|ALTERNATIVES:|$)'
        match = re.search(risk_section_pattern, text, re.DOTALL | re.IGNORECASE)
        
        if match:
            risk_text = match.group(1)
            
            # Extract specific risk metrics
            detection_prob = self._extract_probability(risk_text, 'detection')
            success_prob = self._extract_probability(risk_text, 'success')
            
            return {
                'detection_probability': detection_prob,
                'success_probability': success_prob,
                'overall_risk': 'medium',
                'risk_factors': self._extract_risk_factors(risk_text)
            }
        
        return {
            'detection_probability': 0.3,
            'success_probability': 0.7,
            'overall_risk': 'medium',
            'risk_factors': []
        }
    
    def _extract_probability(self, text: str, prob_type: str) -> float:
        """Extract specific probability from text"""
        pattern = f'{prob_type}.*?(\d+)%'
        match = re.search(pattern, text, re.IGNORECASE)
        
        if match:
            return int(match.group(1)) / 100.0
        
        return 0.5  # Default 50%
    
    def _extract_risk_factors(self, risk_text: str) -> List[str]:
        """Extract risk factors from risk assessment text"""
        factors = []
        
        if 'detection' in risk_text.lower():
            factors.append('detection_risk')
        if 'failure' in risk_text.lower():
            factors.append('operation_failure')
        if 'exposure' in risk_text.lower():
            factors.append('asset_exposure')
        
        return factors
    
    def _extract_alternatives(self, text: str) -> List[Dict[str, Any]]:
        """Extract alternative strategies"""
        # Simplified extraction - in practice would be more sophisticated
        return [
            {'strategy': 'Alternative approach 1', 'confidence': 0.6},
            {'strategy': 'Fallback option', 'confidence': 0.4}
        ]
    
    def _extract_coordination(self, text: str) -> List[str]:
        """Extract coordination suggestions"""
        coord_pattern = r'(?:COORDINATION:|TEAM:|AGENTS:)(.*?)(?:$|\n\n)'
        match = re.search(coord_pattern, text, re.DOTALL | re.IGNORECASE)
        
        if match:
            coord_text = match.group(1)
            suggestions = [s.strip() for s in coord_text.split('-') if s.strip()]
            return suggestions
        
        return []
    
    def _extract_learning_insights(self, text: str) -> List[str]:
        """Extract learning insights from reasoning"""
        insights = []
        
        # Look for patterns that indicate learning
        if 'adapt' in text.lower():
            insights.append('Adaptation strategy identified')
        if 'pattern' in text.lower():
            insights.append('Behavioral pattern recognized')
        if 'effective' in text.lower():
            insights.append('Effectiveness evaluation noted')
        
        return insights
    
    def _update_learning(self, reasoning_type: ReasoningType, 
                        context: ReasoningContext, result: ReasoningResult):
        """Update learning from reasoning results"""
        # Store reasoning history
        self.reasoning_history.append({
            'type': reasoning_type.value,
            'agent': context.agent_id,
            'team': context.team,
            'confidence': result.confidence_score,
            'timestamp': result.timestamp.isoformat()
        })
        
        # Update adversarial patterns
        if context.adversary_patterns:
            self.adversarial_patterns[context.team].append(context.adversary_patterns)
        
        # Update strategy effectiveness (would be updated based on actual results)
        strategy_key = f"{reasoning_type.value}_{context.team}"
        self.strategy_effectiveness[strategy_key] = result.confidence_score

# Adversarial LLM Framework for competitive reasoning
class AdversarialLLMFramework:
    """Framework for adversarial LLM reasoning between red and blue teams"""
    
    def __init__(self):
        self.red_llm = LLMReasoningEngine("red_team_model")
        self.blue_llm = LLMReasoningEngine("blue_team_model")
        self.game_theory_engine = GameTheoryEngine()
        self.cross_learning_enabled = True
        
        logger.info("Adversarial LLM Framework initialized")
    
    async def adversarial_reasoning_cycle(self, red_context: ReasoningContext, 
                                        blue_context: ReasoningContext) -> Dict[str, Any]:
        """Run one cycle of adversarial reasoning"""
        
        # Red team reasoning
        red_result = await self.red_llm.reason(
            ReasoningType.STRATEGIC_PLANNING, red_context
        )
        
        # Update blue team context with red team intelligence
        if self.cross_learning_enabled:
            blue_context.adversary_patterns = {
                'recent_actions': red_result.recommended_action,
                'strategy_pattern':red_result.reasoning_chain
            }
        
        # Blue team counter-reasoning
        blue_result = await self.blue_llm.reason(
            ReasoningType.DEFENSE_COORDINATION, blue_context
        )
        
        # Game theory analysis
        game_analysis = self.game_theory_engine.analyze_strategies(
            red_result, blue_result
        )
        
        return {
            'red_reasoning': red_result,
            'blue_reasoning': blue_result,
            'game_analysis': game_analysis,
            'predicted_outcome': game_analysis.get('predicted_winner'),
            'adaptation_suggestions': game_analysis.get('adaptations', [])
        }
    
    def enable_cross_pollination(self):
        """Enable agents to learn from each other's failures"""
        self.cross_learning_enabled = True
        
        # Share successful strategies between opposing teams
        # (This creates an evolutionary pressure for better strategies)
        red_strategies = self.red_llm.strategy_effectiveness
        blue_strategies = self.blue_llm.strategy_effectiveness
        
        # Cross-pollinate learning (simplified implementation)
        for strategy, effectiveness in red_strategies.items():
            if effectiveness > 0.8:  # Highly effective strategies
                # Blue team learns to counter this strategy
                counter_strategy = f"counter_{strategy}"
                self.blue_llm.strategy_effectiveness[counter_strategy] = 0.7
        
        logger.info("Cross-pollination enabled - agents learning from adversaries")

class GameTheoryEngine:
    """Game theory analysis for strategic decision making"""
    
    def analyze_strategies(self, red_result: ReasoningResult, 
                          blue_result: ReasoningResult) -> Dict[str, Any]:
        """Analyze strategies using game theory principles"""
        
        # Simplified Nash equilibrium analysis
        red_payoff = self._calculate_payoff(red_result, 'offensive')
        blue_payoff = self._calculate_payoff(blue_result, 'defensive')
        
        # Determine predicted outcome
        if red_payoff > blue_payoff * 1.2:
            predicted_winner = 'red_team'
        elif blue_payoff > red_payoff * 1.2:
            predicted_winner = 'blue_team'
        else:
            predicted_winner = 'stalemate'
        
        return {
            'red_payoff': red_payoff,
            'blue_payoff': blue_payoff,
            'predicted_winner': predicted_winner,
            'equilibrium_stability': self._assess_stability(red_payoff, blue_payoff),
            'adaptations': self._suggest_adaptations(red_result, blue_result)
        }
    
    def _calculate_payoff(self, result: ReasoningResult, strategy_type: str) -> float:
        """Calculate strategy payoff"""
        base_payoff = result.confidence_score
        
        # Adjust based on risk assessment
        risk_factor = 1.0 - result.risk_assessment.get('detection_probability', 0.5)
        success_factor = result.risk_assessment.get('success_probability', 0.5)
        
        payoff = base_payoff * risk_factor * success_factor
        
        return payoff
    
    def _assess_stability(self, red_payoff: float, blue_payoff: float) -> str:
        """Assess Nash equilibrium stability"""
        payoff_diff = abs(red_payoff - blue_payoff)
        
        if payoff_diff < 0.1:
            return 'stable'
        elif payoff_diff < 0.3:
            return 'moderately_stable'
        else:
            return 'unstable'
    
    def _suggest_adaptations(self, red_result: ReasoningResult, 
                           blue_result: ReasoningResult) -> List[str]:
        """Suggest strategic adaptations"""
        adaptations = []
        
        # Analyze strategy effectiveness
        if red_result.confidence_score > blue_result.confidence_score:
            adaptations.append("Blue team should increase defensive measures")
            adaptations.append("Blue team should improve threat detection")
        else:
            adaptations.append("Red team should diversify attack vectors")
            adaptations.append("Red team should improve stealth techniques")
        
        return adaptations

# Example usage
if __name__ == "__main__":
    async def test_llm_reasoning():
        """Test the LLM reasoning system"""
        engine = LLMReasoningEngine()
        
        # Create test context
        context = ReasoningContext(
            agent_id="red_agent_1",
            team="red",
            specialization="penetration_tester",
            current_objective="Compromise database server",
            available_tools=["nmap", "metasploit", "sqlmap"],
            network_state={
                "hosts": ["web-server", "db-server"],
                "compromised_hosts": [],
                "asset_values": {"db-server": 2000000}
            },
            threat_landscape={"active_threats": 2},
            historical_actions=[],
            adversary_patterns={},
            time_pressure=0.7,
            risk_tolerance=0.6
        )
        
        # Test reasoning
        result = await engine.reason(ReasoningType.STRATEGIC_PLANNING, context)
        
        print("LLM Reasoning Test Results:")
        print(f"Reasoning steps: {len(result.reasoning_chain)}")
        print(f"Confidence: {result.confidence_score:.2f}")
        print(f"Recommended action: {result.recommended_action['type']}")
        print(f"Risk assessment: {result.risk_assessment}")
    
    # Run test
    asyncio.run(test_llm_reasoning())